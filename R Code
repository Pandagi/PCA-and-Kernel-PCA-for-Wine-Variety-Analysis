---
title: "STATS 767 Project"
author: "Huanchao Qin"
output:
  html_document:
    toc: true
    toc_float: true
    df_print: paged
    theme: readable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(nnet)
```


### Data preprocessing
# read the data into R
df = read.csv("Wine.csv")
# convert the response variable to a factor
df$Customer_Segment = factor(df$Customer_Segment)
# have a glance at the data
str(df)
# scale the data
scale_df = data.frame(scale(df[,-14]))
# split the dataset into the training and test set
library(caTools)
# Set the seed for reproducibility
set.seed(66)
# specify the proportion of data to allocate for the training set 
split = sample.split(df$Customer_Segment, SplitRatio = 0.7)

### Standard PCA
# perform linear principal components analysis on the scaled data
linear_PCA = prcomp(scale_df)
# make the scree plot
screeplot(linear_PCA, main='Screeplot for standard PCA', xlab='component')
sum(linear_PCA$sdev^2)
# proportion of the variability 
round(linear_PCA$sdev^2/sum(linear_PCA$sdev^2),2)
# compute the correlation of the first principal component score 
# with the original variables
round(linear_PCA$rotation[,1] * linear_PCA$sdev[1],2)
# compute the correlation of the second principal component score 
# with the original variables
round(linear_PCA$rotation[,2] * linear_PCA$sdev[2],2)
# compute the correlation of the third principal component score 
# with the original variables
round(linear_PCA$rotation[,3] * linear_PCA$sdev[3],2)
# combine the principal component scores and add the label
linear_pca = cbind(data.frame(linear_PCA$x),cultivar=df$Customer_Segment)
# get the transformed training and test sets
training_set_linear = subset(linear_pca, split == TRUE)
test_set_linear = subset(linear_pca, split == FALSE)

# Setting the reference
training_set_linear$cultivar = relevel(training_set_linear$cultivar, ref = "1")
#fitting training data to the Multinomial Regression Model
model_linear = multinom(cultivar ~ ., data = training_set_linear)
# Predicting the values for the test dataset
y_pred_linear = predict(model_linear, newdata = test_set_linear[,-14], "class")
#The Confusion Matrix
(tab_linear = table("real"=test_set_linear$cultivar, "pred" = y_pred_linear))
# Calculating accuracy - sum of diagonal elements divided by total obs
round((sum(diag(tab_linear))/sum(tab_linear))*100,2)

### Gaussian RBF kernel PCA
# apply Kernel PCA
library(kernlab)

# tuning the parameters
sigma_values = seq(0.001, 2, by = 0.001)
best_accuracy = 0
best_sigma = 0

for (sigma in sigma_values) {
  
# Radial Basis kernel function "Gaussian"
G_PCA = kpca(~., data = scale_df,kernel="rbfdot",kpar=list(sigma=sigma))
# get the principal component scores 
G_pca = as.data.frame(predict(G_PCA, scale_df))
# label the data
G_pca$cultivar = df$Customer_Segment
# get the transformed training and test sets
training_set_G = subset(G_pca, split == TRUE)
test_set_G = subset(G_pca, split == FALSE)

# Setting the reference
training_set_G$cultivar = relevel(training_set_G$cultivar, ref = "1")
#fitting training data to the Multinomial Regression Model
model_G = multinom(cultivar ~ ., data = training_set_G)
# Predicting the values for the test dataset
y_pred_G = predict(model_G, newdata = test_set_G[,-ncol(training_set_G)], 
                   "class")
#The Confusion Matrix
tab_G = table("real"=test_set_G$cultivar, "pred" = y_pred_G)
# Calculating accuracy - sum of diagonal elements divided by total obs
current_accuracy = round((sum(diag(tab_G))/sum(tab_G))*100,2)
# Keep track of the best scale, offset, and accuracy
    if (current_accuracy > best_accuracy) {
      best_accuracy = current_accuracy
      best_sigma = sigma
    }
  }
best_accuracy
best_sigma

# after tuning, select the approriate kernel function
# Radial Basis kernel function "Gaussian"
G_PCA = kpca(~., data = scale_df,kernel="rbfdot",kpar=list(sigma=0.003))
# get the principal component scores 
G_pca = as.data.frame(predict(G_PCA, scale_df))
# label the data
G_pca$cultivar = df$Customer_Segment
# get the transformed training and test sets
training_set_G = subset(G_pca, split == TRUE)
test_set_G = subset(G_pca, split == FALSE)

# Setting the reference
training_set_G$cultivar = relevel(training_set_G$cultivar, ref = "1")
#fitting training data to the Multinomial Regression Model
model_G = multinom(cultivar ~ ., data = training_set_G)
# Predicting the values for the test dataset
y_pred_G = predict(model_G, newdata = test_set_G[,-ncol(training_set_G)], 
                   "class")
#The Confusion Matrix
(tab_G = table("real"=test_set_G$cultivar, "pred" = y_pred_G))
# Calculating accuracy - sum of diagonal elements divided by total obs
round((sum(diag(tab_G))/sum(tab_G))*100,2)
# number of principal components
ncol(training_set_G) - 1  

### Polynomial Kernel PCA

# tuning the parameter
degree_values = 2:10
best_accuracy = 0
best_degree = 0

for (degree in degree_values) {
  
# Polynomial kernel function
P_PCA = kpca(~., data = scale_df,kernel="polydot",kpar=list(degree=degree))
# get the principal component scores 
P_pca = as.data.frame(predict(P_PCA, scale_df))
# label the data
P_pca$cultivar = df$Customer_Segment
# get the transformed training and test sets
training_set_P = subset(P_pca, split == TRUE)
test_set_P = subset(P_pca, split == FALSE)

# Setting the reference
training_set_P$cultivar = relevel(training_set_P$cultivar, ref = "1")
#fitting training data to the Multinomial Regression Model
model_P = multinom(cultivar ~ ., data = training_set_P)
# Predicting the values for the test dataset
y_pred_P = predict(model_P, newdata = test_set_P[,-ncol(training_set_P)], 
                   "class")
#The Confusion Matrix
tab_P = table("real"=test_set_P$cultivar, "pred" = y_pred_P)
# Calculating accuracy - sum of diagonal elements divided by total obs
current_accuracy = round((sum(diag(tab_P))/sum(tab_P))*100,2)

 # Keep track of the best scale, offset, and accuracy
    if (current_accuracy > best_accuracy) {
      best_accuracy = current_accuracy
      best_degree = degree
    }
  }
best_accuracy
best_degree

# after tuning, select the approriate kernel function
# Polynomial kernel function
P_PCA = kpca(~., data = scale_df,kernel="polydot",kpar=list(degree=2))
# get the principal component scores 
P_pca = as.data.frame(predict(P_PCA, scale_df))
# label the data
P_pca$cultivar = df$Customer_Segment
# get the transformed training and test sets
training_set_P = subset(P_pca, split == TRUE)
test_set_P = subset(P_pca, split == FALSE)

# Setting the reference
training_set_P$cultivar = relevel(training_set_P$cultivar, ref = "1")
#fitting training data to the Multinomial Regression Model
model_P = multinom(cultivar ~ ., data = training_set_P)
# Predicting the values for the test dataset
y_pred_P = predict(model_P, newdata = test_set_P[,-ncol(training_set_P)], 
                   "class")
#The Confusion Matrix
(tab_P = table("real"=test_set_P$cultivar, "pred" = y_pred_P))
# Calculating accuracy - sum of diagonal elements divided by total obs
round((sum(diag(tab_P))/sum(tab_P))*100,2)
# number of principal components
ncol(training_set_P) - 1

### Laplacian Kernel PCA

# tuning the parameter
sigma_values = seq(0.001, 1, by = 0.001)
best_accuracy = 0
best_sigma = 0

for (sigma in sigma_values) {

# Laplacian kernel function
L_PCA = kpca(~., data = scale_df,kernel="laplacedot",kpar=list(sigma = sigma))
# get the principal component scores 
L_pca = as.data.frame(rotated(L_PCA))
# label the data
L_pca$cultivar = df$Customer_Segment
# get the transformed training and test sets
training_set_L = subset(L_pca, split == TRUE)
test_set_L = subset(L_pca, split == FALSE)

# Setting the reference
training_set_L$cultivar = relevel(training_set_L$cultivar, ref = "1")
#fitting training data to the Multinomial Regression Model
model_L = multinom(cultivar ~ ., data = training_set_L)
# Predicting the values for the test dataset
y_pred_L = predict(model_L, newdata = test_set_L[,-ncol(training_set_L)], 
                   "class")
#The Confusion Matrix
tab_L = table("real"=test_set_L$cultivar, "pred" = y_pred_L)
# Calculating accuracy - sum of diagonal elements divided by total obs
current_accuracy = round((sum(diag(tab_L))/sum(tab_L))*100,2)

# Keep track of the best scale, offset, and accuracy
    if (current_accuracy > best_accuracy) {
      best_accuracy = current_accuracy
      best_sigma = sigma
    }
  }

best_accuracy
best_sigma

# after tuning, select the approriate kernel function
# Laplacian kernel function
L_PCA = kpca(~., data = scale_df,kernel="laplacedot",kpar=list(sigma = 0.001))
# get the principal component scores 
L_pca = as.data.frame(rotated(L_PCA))
# label the data
L_pca$cultivar = df$Customer_Segment
# get the transformed training and test sets
training_set_L = subset(L_pca, split == TRUE)
test_set_L = subset(L_pca, split == FALSE)

# Setting the reference
training_set_L$cultivar = relevel(training_set_L$cultivar, ref = "1")
#fitting training data to the Multinomial Regression Model
model_L = multinom(cultivar ~ ., data = training_set_L)
# Predicting the values for the test dataset
y_pred_L = predict(model_L, newdata = test_set_L[,-ncol(training_set_L)],
                   "class")
#The Confusion Matrix
(tab_L = table("real"=test_set_L$cultivar, "pred" = y_pred_L))
# Calculating accuracy - sum of diagonal elements divided by total obs
round((sum(diag(tab_L))/sum(tab_L))*100,2)
# number of principal components
ncol(training_set_L) - 1

### Sigmoid Kernel PCA

# tuning parameters
scales = seq(0.1, 1, by = 0.1)
offsets = seq(-1, 1, by = 0.1)

best_accuracy = 0
best_scale = 0
best_offset = 0

for (scale in scales) {
  for (offset in offsets) {
    
    # Perform Sigmoid Kernel PCA
    S_PCA = kpca(~., data=scale_df,kernel="tanhdot",
                 kpar=list(scale=scale,offset=offset))
    # get the principal component scores 
    S_pca = as.data.frame(rotated(S_PCA))
    # label the data
    S_pca$cultivar = df$Customer_Segment
    # get the transformed training and test sets
    training_set_S = subset(S_pca, split == TRUE)
    test_set_S = subset(S_pca, split == FALSE)
    
    # Setting the reference
    training_set_S$cultivar = relevel(training_set_S$cultivar, ref = "1")
    #fitting training data to the Multinomial Regression Model
    model_S = multinom(cultivar ~ ., data = training_set_S)
    # Predicting the values for the test dataset
    y_pred_S = predict(model_S, newdata = test_set_S[,-ncol(training_set_S)], 
                       "class")
   #The Confusion Matrix
   tab_S = table("real"=test_set_S$cultivar, "pred" = y_pred_S)
   # Calculating accuracy - sum of diagonal elements divided by total obs
   current_accuracy  = round((sum(diag(tab_S))/sum(tab_S))*100,2)
        
    # Keep track of the best scale, offset, and accuracy
    if (current_accuracy > best_accuracy) {
      best_accuracy = current_accuracy
      best_scale = scale
      best_offset = offset
    }
  }
}

best_accuracy
best_scale
best_offset

# after tuning, select the approriate kernel function
# Perform Sigmoid Kernel PCA
S_PCA = kpca(~., data=scale_df,kernel="tanhdot",kpar=list(scale=0.2,offset=0.1))
# get the principal component scores 
S_pca = as.data.frame(rotated(S_PCA))
# label the data
S_pca$cultivar = df$Customer_Segment
# get the transformed training and test sets
training_set_S = subset(S_pca, split == TRUE)
test_set_S = subset(S_pca, split == FALSE)
    
# Setting the reference
training_set_S$cultivar = relevel(training_set_S$cultivar, ref = "1")
#fitting training data to the Multinomial Regression Model
model_S = multinom(cultivar ~ ., data = training_set_S)
# Predicting the values for the test dataset
# Predicting the values for the test dataset
y_pred_S = predict(model_S, newdata = test_set_S[,-ncol(training_set_S)],
                   "class")
#The Confusion Matrix
(tab_S = table("real"=test_set_S$cultivar, "pred" = y_pred_S))
# Calculating accuracy - sum of diagonal elements divided by total obs
round((sum(diag(tab_S))/sum(tab_S))*100,2)  
# number of principal components
ncol(training_set_S) - 1

### Visualization results
# plot
SP = data.frame(X1=linear_PCA$x[,1],X2=linear_PCA$x[,2],
                kernel=rep("Standard PCA",178),cultivar = df$Customer_Segment)
G = data.frame(rotated(G_PCA)[,1:2],kernel=rep("Gaussian RBF kernel",178),
               cultivar = df$Customer_Segment)
P = data.frame(rotated(P_PCA)[,1:2],kernel=rep("Polynomial Kernel",178),
               cultivar = df$Customer_Segment)
L = data.frame(rotated(L_PCA)[,1:2],kernel=rep("Laplacian Kernel",178),
               cultivar = df$Customer_Segment)
S = data.frame(rotated(S_PCA)[,1:2],kernel=rep("Sigmoid Kernel",178),
               cultivar = df$Customer_Segment)
ALL = rbind(SP,G,P,L,S)

split_data = split(ALL, ALL$kernel)  

library(ggplot2)
plot_list = lapply(names(split_data), function(kernel) {
  ggplot(split_data[[kernel]], aes(x = X1, y = X2,color=cultivar)) +
    geom_point(size=.5) +
    labs(x = "PC1", y = "PC2",title=kernel)  
})
# Arrange the scatter plots in a grid
gridExtra::grid.arrange(grobs = plot_list, nrow = 3)
